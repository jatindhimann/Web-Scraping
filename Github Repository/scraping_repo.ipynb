{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# get repo name\n",
    "def get_repo_name(doc):\n",
    "    repo_head_tag = doc.find_all('a', {'itemprop': 'name codeRepository'})\n",
    "    repo_names = []\n",
    "    for i in range(len(repo_head_tag)):\n",
    "        repo_names.append(repo_head_tag[i].text.strip())\n",
    "    return repo_names\n",
    "\n",
    "# get stars\n",
    "def get_stars(doc):\n",
    "    star_fork_div_tag = doc.find_all('div', {'class': 'f6 color-fg-muted mt-2'})\n",
    "    stars = []\n",
    "    for i in range(len(star_fork_div_tag)):\n",
    "        star_fork_content = str(star_fork_div_tag[i])\n",
    "        star_ = BeautifulSoup(star_fork_content, 'html.parser')\n",
    "        star = star_.find_all('a', {'class': 'Link--muted mr-3'})\n",
    "        if len(star) > 0:\n",
    "            stars.append(star[0].text.strip())\n",
    "        else:\n",
    "            stars.append(0)\n",
    "    return stars\n",
    "\n",
    "# get forks\n",
    "def get_forks(doc):\n",
    "    star_fork_div_tag = doc.find_all('div', {'class': 'f6 color-fg-muted mt-2'})\n",
    "    forks = []\n",
    "    for i in range(len(star_fork_div_tag)):\n",
    "        star_fork_content = str(star_fork_div_tag[i])\n",
    "        fork_ = BeautifulSoup(star_fork_content, 'html.parser')\n",
    "        fork = fork_.find_all('a', {'class': 'Link--muted mr-3'})\n",
    "        if len(fork) > 1:\n",
    "            forks.append(fork[1].text.strip())\n",
    "        else:\n",
    "            forks.append(0)\n",
    "    return forks\n",
    "\n",
    "# get repo url\n",
    "def get_repo_url(doc,base_url):\n",
    "    repo_name = get_repo_name(doc)\n",
    "    repo_url = []\n",
    "    repo_head_tag = doc.find_all('a', {'itemprop': 'name codeRepository'})\n",
    "    for i in range(len(repo_head_tag)):\n",
    "        repo_url.append(base_url + repo_name[i])\n",
    "    return repo_url\n",
    "\n",
    "\n",
    "def scrape_github_id(repo_url):\n",
    "    idx = 0\n",
    "\n",
    "    while repo_url[idx] != '?':\n",
    "        idx = idx + 1\n",
    "\n",
    "    base_url = repo_url[:idx] + '/'\n",
    "\n",
    "    response = requests.get(repo_url)\n",
    "    if response.status_code == 200:\n",
    "        page_contents = response.text\n",
    "        doc = BeautifulSoup(page_contents, 'html.parser')\n",
    "        repos = {'Repository Name': get_repo_name(doc), 'Stars': get_stars(doc), 'Forks': get_forks(doc), 'URL': get_repo_url(doc,base_url)}\n",
    "        return pd.DataFrame(repos)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def Mega_scrape(repo_url):\n",
    "    idx = 0\n",
    "\n",
    "    while repo_url[idx] != '?':\n",
    "        idx = idx + 1\n",
    "\n",
    "    if '&' not in repo_url:\n",
    "        repo_url = repo_url[0:idx+1] + '&' + repo_url[idx+1:]\n",
    "    \n",
    "    initials = repo_url[0:idx+1]\n",
    "    finals = repo_url[idx+1:]\n",
    "\n",
    "    response = requests.get(repo_url)\n",
    "    page_contents = response.text\n",
    "    doc = BeautifulSoup(page_contents, 'html.parser')\n",
    "    repo_tags = doc.find_all('span',{'class':'Counter'})\n",
    "\n",
    "    no_of_repo = repo_tags[0].text\n",
    "    no_of_pages = int(no_of_repo)/30\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for i in range(int(no_of_pages + 1)):\n",
    "        print('Scraping {} Repository Page'.format(i + 1))\n",
    "        new_repo_url = initials + 'page={}'.format(i + 1) + finals\n",
    "        df = scrape_github_id(new_repo_url)\n",
    "        dfs.append(df)\n",
    "\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    final_df.to_csv('repository_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = 'https://github.com/john-smilga?tab=repositories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1 Repository Page\n",
      "Scraping 2 Repository Page\n",
      "Scraping 3 Repository Page\n",
      "Scraping 4 Repository Page\n",
      "Scraping 5 Repository Page\n",
      "Scraping 6 Repository Page\n",
      "Scraping 7 Repository Page\n",
      "Scraping 8 Repository Page\n",
      "Scraping 9 Repository Page\n"
     ]
    }
   ],
   "source": [
    "Mega_scrape(repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
